\chapter{Introduction}
\label{chap:intro}

Machine learning can provide methods for learning controllers for robotic tasks. Yet, even with recent advances in this field, the problem of automatically designing and learning controllers for robots, especially bipedal robots, remains a difficult problem. Some of the core challenges of learning for control scenarios can be summarized as follows: It is expensive to do learning experiments that require a large number of samples with physical robots. Specifically, legged robots are not robust to falls and failures, and are time-consuming to work with and repair. Furthermore, commonly used cost functions for learning controllers are noisy to evaluate, non-convex and non-differentiable. In order to find learning approaches that can be used on real robots, it is thus important to keep these considerations in mind.

Deep reinforcement learning approaches can deal with noise, discontinuities and non-convexity of the objective, but they are not data-efficient. These approaches could take on the order of a million samples to learn locomotion controllers~\citep{peng2016terrain}, which would be infeasible on a real robot. For example, on the ATRIAS robot, $10,000$ samples would take $7$ days, in theory. But practically, the robot needs to be ``reset" between trials and repaired in case of damage, which has taken up to months. 

Using structured expert-designed policies can help minimize damage to the robot and make the search for successful controllers faster. In fact, robot controllers often consist of many expert designed heuristics, for example feedback control of the Center of Mass (CoM) position and velocity, feedback control of robot joints, and designing reference trajectories. State of the art works in bipedal robots featuring such heuristics include \cite{feng2015optimization}, \cite{kuindersma2016optimization} and \cite{hubicki2016walking}. These heuristics consist of sets of inter-dependent parameters, which can be hard to tune, especially in higher dimensions. This complexity motivates methods for learning parameters automatically. However, the learning problem is still black-box, non-convex and discontinuous. This eliminates approaches like PI$^2$~\citep{theodorou2010generalized} which make assumptions about the dynamics of the system and PILCO~\citep{deisenroth2011pilco} which assumes a continuous cost landscape. Evolutionary approaches like CMA-ES~\citep{hansen2006cma} can also be prohibitively expensive, needing in the order of thousands of samples \citep{song2015neural}.

In comparison, Bayesian optimization (BO) is a sample-efficient optimization technique that is robust to non-convexity, noise and even discontinuities. It has been recently used in a range of robotics problems, such as~\citet{calandra2016bayesian}, ~\citet{marco2017virtual}, ~\citet{cully2015robots}. However, sample-efficiency of conventional BO degrades in high dimensions, even for dimensionalities commonly encountered in locomotion controllers. Because of this, hardware-only learning becomes intractable for flexible controllers and complex robots. One way of addressing this issue is to utilize simulation to aid learning of controller parameters. However, simulation-only learning is vulnerable to learning policies that exploit the simulation and perform well in simulation but poorly on the actual robot due to simulation inaccuracies. 
This motivates the development of approaches that can incorporate simulation-based information into the learning method, then optimize with few samples on hardware. 

%One popular data-efficient method for learning controller parameters is Bayesian Optimization (BO). BO is a sample-efficient gradient-free black-box optimization method that has been applied to a wide range of robotics problems. For example, \cite{Calandra2016}, \cite{marco2017virtual}, \cite{cully2015robots} try to learn parameters directly on hardware using BO. However, the performance of BO degrades in high dimensions (see~\cite{localBO17} for a related discussion), even for dimensionalities commonly encountered in locomotion controllers. We aim to overcome this problem by incorporating domain knowledge into BO.
\begin{figure}
    \centering
    \includegraphics[width = 0.4\textwidth]{img/atrias1.jpg}
    %\includegraphics[width = 0.35\textwidth]{img/atrias2.jpg}
    \includegraphics[width = 0.4\textwidth]{img/atrias_dog5d_walking_around_boom.jpg}
    \caption{\small{Our testbed is CMU's ATRIAS robot. 
    %While ATRIAS can walk in 3D, our experiments are focused on walking around a boom.
    }}
        \vspace{-5mm}
    \label{fig:atrias}
\end{figure}
%In our previous work, we proposed a transformation based on domain knowledge that reparameterized human-like walking controllers based on their behaviour in a high-fidelity simulation.
%The main idea was that some behavioural cues have a higher probability of transferring between simulation and hardware than exact cost or controller performance. For example, a controller that falls instantly in simulation by tilting its torso, would have a small chance of walking on hardware. On the other hand, a controller that walks efficiently in simulation might have a higher chance of walking on hardware, albeit with a different cost. Hence, it makes sense to roughly group parameters based on the behaviour they produce in simulation, and use this as a distance metric to distinguish between them. 

The idea of using simulation performance to speed up optimization on hardware has been explored before. A common approach is to learn controllers in simulation, and use this as a starting point on hardware. A domain expert would then typically have to fine-tune parameters on hardware. \cite{mordatch2015ensemble}~learn parameters using a collection of simulations that help account for model uncertainty. \cite{ha2015reducing}~iteratively learn both the model and controller parameters using the differences between simulated behaviors and observed hardware experiments. \cite{wilson2014using} use evaluations from simulation as a noisy prior for the optimization on hardware. \cite{cully2015robots} pre-select high performing controllers from simulation and search among them on hardware. While all of these are very promising approaches, they usually suffer when simulation is significantly different from hardware.
 
In this thesis, we present a framework that uses information from high-fidelity simulators to learn sample-efficiently on hardware. We use simulation to build informed feature transforms that are used to measure similarity during BO. Thus, the similarity between controller parameters, during optimization on hardware, is informed by how they perform in simulation. With this, it becomes possible to quickly infer which regions of the input space are likely to perform well on hardware. This method has been tested on the ATRIAS biped robot (Figure~\ref{fig:atrias}) and shows considerable improvement in sample-efficiency over traditional BO. Furthermore, we present a procedure for systematically evaluating robustness of such approaches to simulation-hardware mismatch. We utilize hardware data to build a mismatch estimate between simulation and hardware and use it to update our faetures from simulation. We also conduct extensive comparisons with competitive baselines from related work, such as~\cite{cully2015robots} and \cite{wilson2014using}. 

In cases where well-performing expert-designed policies are hard to obtain, it might be useful to learn high-dimensional over-parametric neural network policies. A popular approach for learning neural network policies in simulation and transferring to hardware is domain randomization. \cite{tan2018sim} apply this to a quadrupedal robot and learn gaits that transfer to hardware. However, for bipedal robots such learning would lead to overly conservative controllers, such as in \cite{mordatch2015ensemble}. In this thesis, we study the effect of different policy structures and their effect on the transfer of neural network policies learned in simulation to hardware, without domain randomization. We find that expert-designed structure can improve the rate of transfer, and enables a shared autonomy between the neural network and user. While the neural network improves the expert policy, the user can still control the overall behavior, and modify it for a slightly different setting without any retraining.

The rest of this thesis is organized as follows: the rest of this chapter introduces the main ideas used in this thesis. Chapter \ref{chap:back} gives background details about Bayesian optimization and reinforcement learning. Chapter \ref{chap:learn} introduces the feature transforms that help transfer data between simulation and hardware. Chapter \ref{chap:robots} introduces the robots, controllers and describes experiments with feature transforms. Chapter \ref{chap:deep} describes the deep reinforcement learning set up and experiments. Finally, Chapter \ref{chap:conclusions} discusses the contributions of this work and proposes related future work.

\section{Learning to learn from simulation}

As described above, information from simulation can be added in the form of noisy priors or models \citep{wilson2014using}, pre-selected promising controllers \citep{cully2015robots}, or as a starting point for hardware learning \citep{endo2008learning}. While all these are promising approaches, they can typically suffer when the simulation is significantly different from hardware. 
\textbf{We propose incorporating simulation knowledge by building informed feature transforms that project the higher dimensional controller parameter space to an easy to optimize lower dimensional space of features.} These features essentially provide a bias that can be overcome if simulation is a poor representation of hardware, by propagating back information from hardware. We present two ways of designing such features -- hand-designed and learned from data. Once a mapping is learned from the original controller space to this feature space in simulation, we can use it to learn sample-efficiently on hardware.


 \subsection{Expert-designed feature transforms}
 \textbf{We use biomechanically inspired features to develop a feature transform for bipedal walking that roughly groups controllers based on their behavior in short simulations.} The objective of such a transformation is that some behavioural cues have a higher probability of transferring between simulation and hardware than exact cost or controller performance. For example, a controller that falls instantly in simulation by tilting its torso, would have a small chance of walking on hardware. On the other hand, a controller that walks efficiently in simulation might have a higher chance of walking on hardware, albeit with a different cost. Hence, it seems reasonable to roughly group parameters based on the behaviour they produce in simulation, and use this as a distance metric to distinguish between them on hardware. Despite being motivated by human walking features, these features also generalize to other robot morphologies and controllers, like the ATRIAS biped. 
 
 
 \subsection{Data-driven feature transforms}
Using extensive expert knowledge, as suggested above, has several advantages. It helps us learn and design features fast as well as provides transparency as to what the learning approach is prioritizing. However, it does raise concerns about problems for which such knowledge might not be available or as easily implementable. With this in mind, \textbf{we also develop a method to construct an informed metric automatically, without relying heavily on domain experts}. We propose to learn a distance metric with a neural network, utilizing data obtained from a high-fidelity simulator. This involves first running short simulations of a locomotion controller on a large grid of control parameters and recording the behavior of each set of parameters. The neural network then learns a mapping between input controller parameters and simulation output/behavior. We propose two ways of defining the target to be learned by the network. The first approach is based on the cost function that is to be optimized with Bayesian optimization on hardware. The second is cost-agnostic: learning to reconstruct a summary of robot trajectories obtained from simulation. This provides a useful re-parameterization: controller parameters that produce similar walking trajectory summaries are closer in this re-parameterized space. 
 

\section{Accounting for mismatch between hardware and simulation}
 An important question that arises when using simulation to guide hardware searches is: how can we learn and take into account the differences between simulation and hardware? Related work includes, \cite{macalpine2016adaptation} who adapt the target task based on the mismatch between the expected and actual performance. \cite{marco2017virtual} develop an automatic way of transitioning between simulation and hardware based on data collected and performance observes so far. 
 
 \textbf{We propose to learn a mismatch-map that represents the mismatch between behaviour encountered in simulation and hardware.} This allows us to build an error map over our parameter space, based on data. We start by trusting all simulation points with predicted mismatch of 0, and as we sample points on hardware, we update our estimate of error on each simulation sample. The advantage of this is that we can have different estimated error values for different regions of parameter space, as some parts of the parameter space might transfer well between hardware and simulation, while others might not. %Using this trust-map, we can achieve significant sample-efficiency even with significantly perturbed simulations. %This is a promising result that is being tested on hardware.
To study the efficacy of this approach, we create a series of increasingly approximate simulators and use features from approximate simulators to optimize controllers on the original simulator. This allows us to empirically examine the deterioration of sample-efficiency as the simulator becomes more and more inaccurate, as well as the advantage of building a mismatch-map to close the loop between simulation and hardware. Our experiments show that this approach can learn controllers when simulation dynamics are perturbed by up to 60\% of their ``true" value. In addition, it is also robust to structured modelling errors, such as unmodelled joint friction, actuator dynamics and other robot components. In comparison, other approaches from the literature that use simulation to speed up hardware experiments, such as \cite{cully2015robots}, suffer when simulation is significantly different from hardware. 


\section{Using neural networks to learn walking policies}

While expert-designed policies are extremely powerful tools for sample-efficiency as well as safety of the robot, it can be challenging to design them in some situations. In such cases, high-dimensional policies like neural networks can be very useful. Typically, these are trained in simulation with deep reinforcement learning, and are shown to be capable of generating very diverse sets of behaviours. For example, \cite{silver2016mastering} use deep reinforcement learning to solve complex long horizon games like Go, starting with no human input. Similar progress has been achieved in the domain of continuous control, dealing with very high dimensional state and action spaces. For example, Deep Deterministic Policy Gradients (DDPG) ~\citep{lillicrap2015continuous} and Trust Region Policy Optimization  (TRPO) ~\citep{schulman2015trust} can solve several control challenges in the Mujoco simulation environment \citep{todorov2012mujoco}. 

While simulation results for deep RL are very impressive, the simulation-hardware gap makes most controllers learned in simulation unsuitable to be implemented on hardware. This is partly because simulations are not perfect representations of real systems, but also because learning approaches often tend to exploit simulation inaccuracies to achieve better performance. 
%Recent work learns parameters of expert controllers on hardware sample-efficiently, for example \cite{antonova2017deep} and \cite{cully2015robots} use Bayesian Optimization. While these are promising, and robust even to hardware damage, they are limited by the controllers they can represent. On the other hand, high-dimensional neural network policies can approximate arbitrarily complex controllers, but too expensive to optimize on hardware. 
One approach to learn policies in simulation and deploy on hardware is domain randomization \citep{mordatch2015ensemble}. It can be used to learn robust neural network policies in simulation by applying random perturbations to the dynamics and other properties of the simulator. This approach has been applied to manipulation problems \citep{peng2017sim}, as well as quadrupedal locomotion problems \citep{tan2018sim}. However, typical domain randomization can lead to controllers that perform worse than controllers trained without randomization \citep{tan2018sim}, as well as make the learning problem much harder \citep{2018arXiv180800177O} taking over 100 years of simulation time to learn successful policies.   

 Domain randomization is especially challenging for under-actuated bipedal robots, as the basin of stability around a controller is typically very small. As a result, it is very difficult to randomly explore the space of controllers to find successful controllers that stabilize a wide range of dynamic models, and other disturbances. Moreover, the learned controllers would typically be quasi-static in nature, similar to \cite{mordatch2015ensemble} in behavior, which is not very impressive in performance. 
 
We experiment with deep reinforcement learning techniques to learn robust controllers in simulation and test them on hardware. Instead of using domain randomization, we create disturbances in simulation by simulating ground height disturbances and study the effect of policy structure on the rate of transfer. Starting with a high-fidelity simulator \citep{martin2015robust}, we experiment with different controller structures, and study their effect on transfer from simulation to hardware, without any domain randomization. The first neural network policy is a general policy that directly outputs the desired actions of the robot, while the second has the structure of an expert-designed policy and predicts the desired states for this policy. We find that structured neural network controllers have a fast training rate in simulation as well as higher rate of transfer to hardware. The structure also enables the user to modify the controller, if required, without having to re-train the neural network from scratch.


%Using our feature transform, we incorporate domain knowledge in BO, and optimize three walking controllers on ATRIAS - two on hardware and all three in simulation. BO equipped with domain knowledge is found to be much more sample-efficient than traditional BO. Using our trust-map, we can further improve sample-efficiency in perturbed simulation experiments. This is a promising result that needs to be tested on hardware in the future.
\section{Summary of completed goals and contributions}

We present evaluations of our proposed methods on the ATRIAS biped robot (Figure \ref{fig:atrias}), ATRIAS simulation, and a 7-link biped simulation. 

First, we use expert designed controllers and optimize their parameters using BO. We evaluate different feature transforms on three  controllers of increasing dimensionality. We start with a 5 dimensional feedback-based reactively stepping controller, and increase its dimensionality to 9. Then we take a 50-dimensional highly non-linear neuromuscular controller and optimize its parameters. We successfully optimize parameters for a 5-dimensional and 9-dimensional controller on the ATRIAS hardware in less than 10 trials, which proves to be challenging for traditional BO. Our results show that these feature transforms - hand-designed and data-driven, extract useful information from simulations, and leads to an effective transfer of knowledge to hardware. 
%This motivates future work for using our approach on hardware for the 50-dimensional controller.

While, Bayesian Optimization benefits from feature transforms and informed distance metrics, we also explore if simulation can be useful in learning other walking policies which might be much higher dimensional. We train two neural network policies - with and without structure and study their rate of transfer to hardware. Our studies show that structured neural network policies have a higher rate of transfer between simulation and hardware as compared to unstructured neural network polices.

The main contributions of this thesis are as below:
\begin{itemize}
    \item A simulation-based feature transform that enables faster optimization of bipedal walking controllers on hardware.
    \item A hybrid optimization approach that integrates model-based information from simulation into a model-free global search framework like Bayesian Optimization
    \item A novel way of updating models from hardware data using feature transforms evaluated on simulation and hardware
    \item A study of the effect of structure on neural network policies trained in simulation and their rate of transfer to hardware
\end{itemize}
%The rest of the paper is organized as follows: In Section \ref{sec:background} we present background on the concepts used in this paper and summarize related work. In Section \ref{sec:dog} we describe our approach of using a locomotion feature transform in detail. Section \ref{sec:atrias_cont} describes our test platform ATRIAS and the controllers used in our experiments. In Section \ref{sec:experiments} we describe our simulation and hardware experiments. Section \ref{sec:conclusions} concludes with further discussion.
