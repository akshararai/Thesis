\chapter{Conclusion}
\label{chap:conclusions}
%\AR{Add a big picture paragraph about learning on hardware.}

In this thesis, we presented different ways of adding simulation information to hardware experiments, with the aim of speeding up learning on hardware. We started with a global black-box optimizer like Bayesian Optimization, and added model-based information into it with the help of feature transforms. We studied different ways of designing these feature transforms, and compared their performance in simulation and hardware. Our results in Chapter \ref{chap:expts} show that our approach can learn controllers on hardware much faster than traditional optimization approaches like BO with SE kernel and CMA-ES on multiple robot morphologies and costs. 

Using simulation for hardware experiments raises the question of the effect of inaccuracies in simulation on the performance of the proposed approaches. To address this, we proposed a way of updating simulation models in a sample-efficient way in the space of features learned in the previous step. We then compared our approach and to approaches from literature on a series of increasingly inaccurate simulators -- with both inaccurate modelling parameters, as well as inaccurate dynamic models. Our experiments in Section \ref{subsec:mismatch_experiments} showed that while other approaches from literature might be able to learn faster with accurate simulators, our approach is more robust to simulation inaccuracies.

Lastly, we studied ways of training neural network policies in simulation and deploying them on hardware. While domain randomization is a popular approach for learning robust policies in simulation, it can lead to very conservative and low performance controllers. We experimented with the effect of structure in neural network policies learned in simulation on the rate of transfer to hardware. We trained two neural network policies on a simulation with ground height disturbances. The first policy was a general neural network with no expert-designed structure, while the second policy was part of a expert policy. Our experiments in Section \ref{sec:nn_expts} show that structured neural network policies can enhance the rate of transfer between simulation and hardware, as well as give the expert user control over the behavior to some extent. We think that this shared autonomy between the neural network and expert is highly desirable, as the neural network improves the expert policy while benefiting from the knowledge of the expert.

In this chapter, we discuss each section of this thesis in some detail, provide some limitations and future directions. We finish by discussing some open questions in learning for robotics, and the application of the work done in this thesis to such questions.

\section{Discussion}
This thesis covered three main ideas : (1) Incorporating simulation information in hardware experiments with the help of feature transforms. (2) Updating these transforms to account for simulation inaccuracies. (3) The use of structure in high-dimensional neural network policies learned in simulation. 
We discuss each of these ideas in some detail next.

\subsection{Feature transforms from simulation in hardware experiments}

Adding simulation information to hardware experiments is well explored in literature. For example, \cite{endo2008learning} learn a policy in simulation and then continue gradient descent on hardware to further optimize the policy. It is also common to hand-tune controllers learned in simulation by an expert.  \cite{abbeel2006using} use learn an inaccurate model of the dynamics from simulation, and continue to improve it from data on hardware. While these approaches are promising when we expect our simulation to be close to hardware, they can be very expensive in terms of number of samples if the simulation is very inaccurate. The bias from simulation can get local optimizers stuck in bad local minima, and it might be difficult or impossible to recover, as demonstrated in Section \ref{subsec:sim_experiments_prior_cully}.

We design a way of incorporating simulation information in a global sample-efficient optimizer on hardware. Instead of adding simulation information to the prior, we add information about simulation trajectories to the kernel. This means that hardware experiments are now guided by the controller behavior recorded in simulation. This transforms the original controller space into a lower dimensional, easier to learn space, where controllers likely to perform well on hardware are close together, and far away from those that are likely to perform poorly.

This lower-dimensional feature space can be hand-designed, in cases where expert knowledge about the task is available. For bipedal locomotion, we describe such features in Section \ref{sec:dog_transform}. On the other hand, raw trajectories, or trajectory summaries from simulation can also be used to create a higher-dimensional but informed feature space. We do this by training a neural network to predict simulation trajectories, as described in Section \ref{sec:approach_traj}. Our hardware experiments on ATRIAS show that both these approaches are able to learn walking controllers very sample-efficiently. They significantly outperform uninformed BO and random sampling. Simulation experiments on a 7-link-biped in Section \ref{sec:7_link_expts} show that these generalize to multiple robot morphologies and controller types.

These experiments showed that features learned in a high-fidelity simulation were able to help improve sample-efficiency of Bayesian optimization during hardware experiments. While domain-informed kernels performed slightly better than the neural network kernels, both learned a 9-dimensional walking controller in less than 10 trials. For a 50-dimensional controller, we showed improvement in simulation. These controllers are still low dimensional as compared to high-dimensional controllers like neural networks. 
In the future, we would like to study if such features can be developed for very high-dimensional policies, possibly enabling these to be directly learned on hardware. 

\subsection{Accounting for simulation inaccuracies}

An important question that arises when using simulation for learning on hardware is : how does performance deteriorate if the simulator is bad? While there are a lot of ways of incorporating simulation information in hardware experiments, there is not a lot of research on studying the effect of inaccurate simulations on these approaches. We develop a systematic way of studying the effect of increasing inaccuracies on the performance of our proposed approach, and compare it to other approaches in literature.

We first study the effect of inaccurate modelling parameters, such as mass, inertia, length and center of mass of each link. These parameters can also be typically identified with system-identification approaches as described in \cite{An:1988:MCR:47324}. However, parameters like friction coefficients and sensor and actuator delay can be variable over experiments, and would need to be re-identified continually. Thus, some amount of robustness to changes in these parameters is ideal. We collect features in an unperturbed simulation, and study optimization performance in \textit{simulated hardware} with up to $\pm50\%$ perturbed inertial parameters. Next, we studied the effect of incorrectly modelled dynamics, such as simplified actuator dynamics and incorrect boom model on the performance of different approaches that transfer information from simulation to hardware. Unmodelled dynamics form a major part of real robot simulators and can be challenging to overcome. In our setting, the high-fidelity original simulator serves as the test model and low-fidelity simulators serve as the source of training data. It is worth noting that the difference between simulation and hardware is not known during test time.

To account for the mismatch between simulation and hardware, we measure the difference between simulation and hardware features, and build a mismatch model from this data. This model lets up update the simulation features in a sample-efficient manner, helping build a better representation of the hardware. Our experiments show that disturbances due to incorrect inertial parameters are in fact easy to overcome with such an update scheme. Features from perturbed simulations find walking controllers in less than 10 trials. On the other hand, inaccurate dynamics models are much harder to compensate for, taking up to 50 trials to learn walking controllers reliably. While other approaches from the literature, like \cite{cully2015robots} can do well when the simulation is close to hardware, they fail to find walking controllers reliably in the presence of large mismatch between simulation and hardware (Figure \ref{fig:cully_prior_sim_versions}). On the other hand adjusted \dogkernel can learn to walk even with large simulation inaccuracies, though it might be less sample-efficient when the simulation is accurate.

Our experiments demonstrated the advantages and disadvantages of using simulation information during hardware experiments. While most approaches are highly sample-efficient when simulation is close to hardware, they suffer when the two are significantly different. This is expected, as the information from simulation is not accurate anymore and the performance of simulation-informed BO can be worse than uninformed BO. On the other hand, adjusted \dogkernel is able to recover from the simulation inaccuracy and guide the search to a region reliable on hardware. Conversely, the adjusted \dogkernel is also less sample-efficient than other approaches when simulation is very close to hardware, as it spends some samples trying to learn about the function and mismatch landscape.

While our evaluations focused on locomotion, the need for incorporating simulation robustly is profound in other of parts robotics, particularly manipulation. Successful modeling of contact dynamics in manipulation is challenging, even for known object/surface types. Utilizing simulation for complex and dynamic manipulation tasks can be useful, since such simulation could be informative in some parts of optimization space, but incorrect in other parts. 
We hope that our experimental setup can be used for analyzing the effectiveness of methods at handling simulation-hardware mismatch in domains other than locomotion, and approaches we evaluate can be adapted to work in other parts of robotics.



\subsection{Effect of structure on high-dimensional neural network policies}

Recently, there has been a lot of interest in learning very high-dimensional neural network policies in simulation using deep reinforcement learning, for example \cite{rajeswaran2017towards}, \cite{peng2017sim}, etc. To transfer these policies to hardware, a common approach is to learn robust policies with the help of domain randomization, as in \cite{mordatch2015ensemble}, \cite{tan2018sim}. These works aim to 
learn robust policies by considering a distribution of model parameters in simulations, which includes the real model of the robot. While this is a very effective way of learning policies that transfer between simulation and hardware, the performance of learned policies is typically poorer than policies learned without domain randomization \citep{tan2018sim}. In our work, we take a different approach to transferability of controllers. Instead of using domain randomization, we explore how structured controllers can eliminate the need to use domain randomization, and lead to a high rate of transfer of policies between simulation and hardware.

We train two neural network policies in simulation - a general NN policy that directly predicts desired forces, and a structured NN policy that predicts set points of a expert-designed policy. Both NN policies are capable of generating arbitrary forces, so there is no loss in generalization with the structure. Our experiments suggest that the structured policy has a much higher rate of transfer between simulation and hardware. Additionally, the structure of the policy lets the user edit the policy for a new, but related, task without retraining the network. This enables a shared autonomy between the NN and the expert, where each benefit from the other.

\cite{peng2017learning} have shown that even in simulation the choice of action space affects the rate of learning as well as disturbance rejection. Building on this, we study the effect of action space, and in particular, the structure of the NN policy on transfer between simulation and hardware. Unsurprisingly, NN policies with an expert-designed structure are able to generalize to unseen state distributions better than a general NN policy without any structure. For example, in most bipedal robots, the initial conditions between simulation and hardware are quite different. The structured NN policy was able to overcome this disturbance and continue walking at the desired speed. On the other hand, the NN policy without structure was not able to handle this case reliably, and failed to walk. Our experiments show that it is indeed possible to learn high-performing policies in simulation that can transfer to hardware without using domain randomization. Expert-designed structured policies are beneficial for two reasons. The NN helps improve the performance of hand-designed heuristics. In addition, the structure enables the user to predict and change the behavior of the NN policy without retraining. 

\section{Future work and open questions}

There are several extensions to the current work, as well as unanswered questions about learning for robotics.

\subsection{Learning general features from data}

We presented two sets of feature transforms for adding simulation information to hardware experiments. So far, the features were either hand-designed or needed expert input in the relevant simulation trajectories. Very often, there were iterations needed where the right features were not selected, or too many features were selected, bringing down the sample-efficiency. Thus, it is important to explore if such features can be learned directly from data, and important characteristics dynamically added or removed based on the problem at hand. 

Features should also include feedback from the environment, for example contact forces. Other than just the end-effector trajectory, the instantaneous forces can also be indicative of the success or failure of particular controllers. Learning low-dimensional representations of these feedback trajectories is an interesting direction for future work. 

Adapting features across multiple tasks and changing lower dimensional representations with context, can be useful for transferring data between different tasks and robots. This requires a sample-efficient way of updating features based on newly observed hardware samples, along with automatically determining the relative importance of features. All of these can together be used to build task-specific lower-dimensional representations that can be learned dynamically, and used to transfer information across multiple tasks.


\subsection{Safe exploration and learning}

A major concern with learning for robots is the safety of the robot. Exploration often involves sampling controllers that could potentially be unstable and damage our robots. Even worse, robots can damage their surroundings, including people and objects that they interact with. Taking safety of a robot and its environment into account is important when using approaches like Bayesian optimization and reinforcement learning.

\cite{berkenkamp2017safe} develop guarantees for model-based reinforcement learning when the dynamics are approximated as a Gaussian process. The policies sampled are guaranteed to be in the region of attraction of an initial stable policy. This is an important step in the direction of building guarantees about safety, but stable initial policies can be hard to find in high-dimensional systems. Building dynamics models might also be prohibitively expensive, and it may be nearly impossible to make them accurate. In such cases, detecting failure or a mismatch between expectation and measurement becomes important. 

We think that starting with conservative policies learned in simulation and slowly building dynamics models is an interesting direction to explore in the future. Exploration while learning dynamics can be done with the help of conservative models and controllers, that build failure maps from expected and measured sensor feedback. All of this together can bring us closer to truly autonomous robots that learn to accomplish goals, starting from inaccurate models and conservative controllers, and slowly learning better controllers along with the dynamics.

\subsection{Transfer learning between controllers}

Given how expensive reinforcement learning typically is, it is important to learn systems that can share information and data. Transfer learning is a popular research direction, with works like \cite{finn2017one} trying to identify task-parameters and learn networks that generalize across multiple tasks. While this is promising, compelling hardware demonstrations of these approaches are few. We believe that automatic relevance determination between task-specific features can be an important step towards transfer learning and data sharing. Different tasks can have different important features, and a correspondingly different cost function. But data from the previous task can be included as if from a different source, as in \cite{poloczek2016multi}. 

In addition, learning problem representations that can generalize across multiple robots can allow for
data transfer across multiple robots. Sharing data in this manner can be extremely helpful in learning to do new tasks sample-efficiently. 

\section{Closing remarks}

This thesis tried to address the problem of fast learning in highly dynamic systems, with the help of simulation. We found that even when the simulator is inaccurate, information from simulation can guide hardware experiments to improve sample-efficiency. We also showed that learning lower-dimensional representations of the controller space in simulation not only helps learn faster on hardware, but also enables us to update the simulation in the same space. 

All of this points towards the advantages of learning representations for optimization and learning. We believe that there are additional advantages to learning representations, like failure detection, safety and transfer learning. We would like to continue exploring these directions in the future.